{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Automated ordering, download, indexing of Landsat USGS data for AGDCv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:08.462774",
     "start_time": "2017-01-04T09:16:48.452330"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, json, requests, time\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Polygon\n",
    "import gzip\n",
    "try:\n",
    "    from urllib.parse import urlparse, urljoin\n",
    "except ImportError:\n",
    "    from urlparse import urlparse, urljoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register with ESPA and enter your credentials below https://espa.cr.usgs.gov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:11.038324",
     "start_time": "2017-01-04T09:17:11.031798"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username = 'bob.slop'\n",
    "password = 'xxxxxx'\n",
    "email = 'bob.slop@ga.gov.au'\n",
    "espa_bulk_downloader = os.path.abspath('C:\\datacube\\code\\agdc-v2\\code\\espa-bulk-downloader\\download_espa_order.py')\n",
    "datacube_home = os.path.abspath('C:\\datacube\\code\\agdc-v2')\n",
    "data_dir = os.path.abspath('D:\\input\\Hyderabad_India')\n",
    "working_dir = os.path.abspath(\"/g/data/u46/users/bxs547\")\n",
    "utils = 'utils\\\\usgslsprepare.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine which WRS path/rows intersect the area of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:14.861344",
     "start_time": "2017-01-04T09:17:14.854893"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ul_lon, ul_lat = 77.83, 17.84 # upper left longitude, latitude\n",
    "lr_lon, lr_lat = 78.00, 17.67 # lower right longitude, latitude\n",
    "date_start, date_end = \"2015-01-11\"  , \"2016-12-12\" # start date and end date for time range selection  \n",
    "sensor_list=[\"olitirs8\", \"tm5\", \"etm7\"] # list of sensors to use\n",
    "\n",
    "polygon_list = [[ul_lon, ul_lat], [lr_lon, ul_lat],[lr_lon, lr_lat],[ul_lon, lr_lat],[ul_lon, ul_lat]]\n",
    "\n",
    "wrs_query = 'http://api.remotepixel.ca/wrs2tiles?search=poly:'+str(polygon_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine which WRS2 path/rows cover the AOI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:21.131986",
     "start_time": "2017-01-04T09:17:18.450750"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "post_query = requests.get(wrs_query)\n",
    "wrs_search_result = json.loads(post_query.text)\n",
    "\n",
    "path_row = []\n",
    "for item in wrs_search_result['results']:\n",
    "    path_row.append(str(item['path'])+\"_\"+str(item['row']))\n",
    "path_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latest Level 1 inventory from USGS to find the available scenes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:22.998768",
     "start_time": "2017-01-04T09:17:22.992187"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_file(url, output_dir):\n",
    "    local_filename = os.path.join(working_dir,url.split('/')[-1])\n",
    "    r = requests.get(url, stream=True)\n",
    "    with open(local_filename, 'wb') as f:\n",
    "        for chunk in r.iter_content(chunk_size=1024): \n",
    "            if chunk: # filter out keep-alive new chunks\n",
    "                f.write(chunk)\n",
    "    return local_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:17:23.651173",
     "start_time": "2017-01-04T09:17:23.647173"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "landsat_csv_list = [\"LANDSAT_8.csv.gz\",\n",
    "                    \"LANDSAT_ETM.csv.gz\",\n",
    "                    \"LANDSAT_ETM_SLC_OFF.csv.gz\",\n",
    "                    \"LANDSAT_TM-1980-1989.csv.gz\",\n",
    "                    \"LANDSAT_TM-1990-1999.csv.gz\",\n",
    "                    \"LANDSAT_TM-2000-2009.csv.gz\",\n",
    "                    \"LANDSAT_TM-2010-2012.csv.gz\"\n",
    "                   ]\n",
    "metadata_file_url = \"https://landsat.usgs.gov/landsat/metadata_service/bulk_metadata_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the inventory data from USGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-23T09:41:03.960570",
     "start_time": "2016-12-23T09:39:11.473261"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for csv in landsat_csv_list:\n",
    "    download_file(urljoin(metadata_file_url, csv), working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unzip inventory files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:26:34.621673",
     "start_time": "2017-01-04T09:17:30.912113"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scene_list = []\n",
    "\n",
    "for csv in landsat_csv_list:\n",
    "    collection = gzip.open(os.path.join(working_dir, csv), 'rb')\n",
    "\n",
    "    data_inventory = pd.read_csv(collection , usecols=['acquisitionDate', \"sceneID\", \"path\", \"row\"]) # limit the columns to only the ones we need\n",
    "    data_inventory[\"path_row\"] = data_inventory[\"path\"].map(str) + \"_\" + data_inventory[\"row\"].map(str)\n",
    "    data_inventory['acquisitionDate'] = data_inventory['acquisitionDate'].apply(pd.to_datetime)\n",
    "    data_inventory = data_inventory.loc[(data_inventory['acquisitionDate'] >= pd.to_datetime(date_start)) &\n",
    "                                        (data_inventory['acquisitionDate'] <= pd.to_datetime(date_end)) &\n",
    "                                        data_inventory['path_row'].isin(path_row)]\n",
    "    scene_list.extend(data_inventory['sceneID'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T09:26:34.629671",
     "start_time": "2017-01-04T09:26:34.623644"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scene_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit an ESPA order for the SR product for the scene list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T11:06:03.007251",
     "start_time": "2017-01-04T11:06:01.928106"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "available_scenes = {}\n",
    "\n",
    "request_data = {\n",
    "    'inputs': scene_list\n",
    "}\n",
    "response = requests.post('https://espa.cr.usgs.gov/api/v0/available-products', json=request_data, auth=(username, password))\n",
    "filtered_scenes = response.json()\n",
    "\n",
    "date_restricted = filtered_scenes.get('date_restricted', {}).get('sr', [])\n",
    "for sensor in sensor_list:\n",
    "    if sensor not in filtered_scenes:\n",
    "        print(\"no inputs for this sensor: \", sensor)\n",
    "        continue\n",
    "    available_scenes[sensor] = [scene for scene in filtered_scenes[sensor]['inputs'] if scene not in date_restricted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T11:06:03.084824",
     "start_time": "2017-01-04T11:06:03.075750"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "json_order=[]\n",
    "for sensor, scenes in available_scenes.items():\n",
    "    order_details={\n",
    "        sensor:\n",
    "        {\n",
    "            \"products\": [\"sr\"],\n",
    "            \"inputs\": scenes,\n",
    "        },\n",
    "        \"format\": \"gtiff\",\n",
    "        \"resize\": {\"pixel_size\": 30, \"pixel_size_units\": \"meters\"}}\n",
    "    json_order.append(order_details)\n",
    "json_order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Submit your order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T11:06:39.982259",
     "start_time": "2017-01-04T11:06:38.159065"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for json_item in json_order:\n",
    "    print('https://espa.cr.usgs.gov/api/v0/order', 'data=',json_item, 'auth=(',username, password,')')\n",
    "    place_order = requests.post('https://espa.cr.usgs.gov/api/v0/order', json=json_item, auth=(username, password))\n",
    "    print(place_order.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T11:07:03.684420",
     "start_time": "2017-01-04T11:07:02.669789"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "current_orders = requests.get('https://espa.cr.usgs.gov/api/v0/list-orders/'+email, auth=(username, password))\n",
    "current_orders.json()['orders']\n",
    "#curl --user username:password https://espa.cr.usgs.gov/api/v0/list-orders/production@email.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-01-04T11:07:11.527527",
     "start_time": "2017-01-04T11:07:09.613585"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for order in current_orders.json()['orders']:\n",
    "    order_status = requests.get('https://espa.cr.usgs.gov/api/v0/order/'+order, auth=(username, password))\n",
    "    status = order_status.json()['status']\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for order_status in current_orders.json()['orders']:\n",
    "    while order_status != 'complete' or order_status != 'purged':\n",
    "        print(\"waiting for order to complete...checking again in 24hrs\")\n",
    "        time.sleep(86400)\n",
    "        Print(\"checking status again\")\n",
    "        current_order_status = requests.get('https://espa.cr.usgs.gov/api/v0/order/simon.oliver@ga.gov.au-12042016-032108-341', auth=(username, password))\n",
    "        print(current_order_status.json()['status'])\n",
    "    print(current_order_status.json()['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### once the order status shows complete - download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if current_order_status.json()['status'] == 'complete'\n",
    "    !python $espa_bulk_downloader -e $email -o ALL -d $data_directory -u $username -p $password    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# organise into scene folders then create a folder for each zip archive and unzip the archive within it\n",
    "#$ for i in `ls /d/input/Hyderabad_India/*/LT5*gz`; do j=`echo $i | cut -c -58`;  tar -xvzf $i -C $j; done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ledaps_prepare_script=os.path.join(datacube_home, utils)\n",
    "print(ledaps_prepare_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_list = [x[0] for x in os.walk(data_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in data_list[1:]:\n",
    "    !python \"C:\\\\datacube\\\\code\\\\agdc-v2\\\\utils\\\\usgslsprepare.py\" $item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the product configurations to the datacube database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "ls5_config=os.path.abspath(\"C:\\\\datacube\\\\code\\\\agdc-v2\\\\docs\\\\config_samples\\\\dataset_types\\\\ls5_scenes.yaml\")\n",
    "ls7_config=os.path.abspath(\"C:\\\\datacube\\\\code\\\\agdc-v2\\\\docs\\\\config_samples\\\\dataset_types\\\\ls7_scenes.yaml\")\n",
    "ls8_config=os.path.abspath(\"C:\\\\datacube\\\\code\\\\agdc-v2\\\\docs\\\\config_samples\\\\dataset_types\\\\ls8_scenes.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove old datacube db instances if they exist and create a new\n",
    "## Weird Ubuntu config issue fixed with sudo ln -s /var/run/postgresql/.s.PGSQL.5432 /tmp/.s.PGSQL.5432\n",
    "!dropdb datacube\n",
    "!createdb datacube \n",
    "# the database to prepare to product addition\n",
    "!datacube system init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# add the product configurations to enable indexing of prepared data\n",
    "!datacube -v product add $ls5_config\n",
    "!datacube -v product add $ls7_config\n",
    "!datacube -v product add $ls8_config\n",
    "!datacube system check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import datacube\n",
    "from datacube.model import Range\n",
    "from datetime import datetime\n",
    "dc = datacube.Datacube(app='dc-example')\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_valid_data as mask_invalid_data\n",
    "import pandas\n",
    "import xarray\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "import geopandas\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import MultiPolygon\n",
    "import rasterio\n",
    "import shapely.geometry\n",
    "import shapely.ops\n",
    "from functools import partial\n",
    "import pyproj\n",
    "from datacube.model import CRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add datasets for the configured products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in data_list[1:]:\n",
    "    item=os.path.join(item,'agdc-metadata.yaml')\n",
    "    if os.path.isfile(item):\n",
    "        !datacube -v dataset add $item --auto-match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "years = list(range(1987,2017))\n",
    "#products = dc.list_products().name\n",
    "products=['ls5_ledaps_scene', 'ls7_ledaps_scene', 'ls8_ledaps_scene']\n",
    "for year in years:\n",
    "    for item in products:\n",
    "        datasets = dc.index.datasets.count(product=item, time=Range(datetime(year, 1, 1), datetime(year+1, 1, 1)))\n",
    "        print(year, item, datasets)\n",
    "    print(\"*****************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datacube.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datasets_union(dss, inputcrs):\n",
    "    thing = shapely.ops.unary_union([shapely.geometry.Polygon(ds.extent.points) for ds in dss])\n",
    "    return shapely.geometry.shape(rasterio.warp.transform_geom(inputcrs, 'EPSG:4326', shapely.geometry.mapping(thing)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def plot_folium(shapes):\n",
    "\n",
    "    mapa = folium.Map(location=[17.3850,78.4867], zoom_start=8)\n",
    "    colors=['#00ff00', '#ff0000', '#00ffff', '#ffffff', '#000000', '#ff00ff']\n",
    "    for shape in shapes:\n",
    "        style_function = lambda x: {'fillColor': '#000000' if x['type'] == 'Polygon' else '#00ff00', \n",
    "                                   'color' : random.choice(colors)}\n",
    "        poly = folium.features.GeoJson(mapping(shape), style_function=style_function)\n",
    "        mapa.add_children(poly)\n",
    "    display(mapa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_folium([datasets_union(dc.index.datasets.search_eager(product='ls5_ledaps_scene'), 'EPSG:32644'),\\\n",
    "             datasets_union(dc.index.datasets.search_eager(product='ls7_ledaps_scene'), 'EPSG:32644'),\\\n",
    "             datasets_union(dc.index.datasets.search_eager(product='ls8_ledaps_scene'), 'EPSG:32644')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dc.list_measurements()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "landsat_rgb_params={'ls8olitirs': {'red': 'sr_band4', 'green': 'sr_band3', 'blue': 'sr_band2', 'product':'ls8_ledaps_scene'},\\\n",
    "                   'ls7etm': {'red': 'sr_band3', 'green': 'sr_band2', 'blue': 'sr_band1', 'product':'ls7_ledaps_scene'},\\\n",
    "                   'ls5tm': {'red': 'sr_band3', 'green': 'sr_band2', 'blue': 'sr_band1', 'product': 'ls5_ledaps_scene'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in landsat_rgb_params.keys():\n",
    "    print(landsat_rgb_params[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHANGE THE SENSOR TO EXECUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sensor = 'ls8olitirs' #ls8olitirs, ls7etm, ls5tm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!set GDAL_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = dc.load(product=landsat_rgb_params[sensor]['product'], \n",
    "              #x=(78.40, 78.57), y=(17.36, 17.52),\n",
    "              x=(77.83, 78.00), y=(17.67, 17.84),\n",
    "              measurements=[landsat_rgb_params[sensor]['red'], landsat_rgb_params[sensor]['green'], landsat_rgb_params[sensor]['blue']],\n",
    "              output_crs='EPSG:32644',resolution=(-30,30))\n",
    "sr = sr.where(sr != -9999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pq = dc.load(product=landsat_rgb_params[sensor]['product'], \n",
    "              #x=(78.40, 78.57), y=(17.36, 17.52),\n",
    "              x=(77.83, 78.00), y=(17.67, 17.84),\n",
    "              measurements=['cfmask'],\n",
    "              output_crs='EPSG:32644',resolution=(-30,30)).cfmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pandas.DataFrame.from_dict(masking.get_flags_def(pq), orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "water = masking.make_mask(pq, cfmask ='water')\n",
    "water.sum('time').plot(cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mask = masking.make_mask(pq, cfmask ='cloud')\n",
    "mask = abs(mask*-1+1)\n",
    "sr_cloudfree = sr.where(mask)\n",
    "mask = masking.make_mask(pq, cfmask ='shadow')\n",
    "mask = abs(mask*-1+1)\n",
    "sr_cloudfree = sr_cloudfree.where(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary view of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sr_cloudfree = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr_cloudfree[landsat_rgb_params[sensor]['red']].plot(col='time', col_wrap=6, robust='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## From http://scikit-image.org/docs/dev/auto_examples/plot_equalize.html\n",
    "import numpy as np\n",
    "from skimage import data, img_as_float\n",
    "from skimage import exposure\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# determine the clip parameters for a target clear (cloud free image) - identified through the index provided\n",
    "def get_p2_p98(rgb, red, green, blue, index):\n",
    "\n",
    "    r = np.nan_to_num(np.array(rgb.data_vars[red][index]))\n",
    "    g = np.nan_to_num(np.array(rgb.data_vars[green][index]))\n",
    "    b = np.nan_to_num(np.array(rgb.data_vars[blue][index]))\n",
    "  \n",
    "    rp2, rp98 = np.percentile(r, (2, 98))\n",
    " \n",
    "    gp2, gp98 = np.percentile(g, (2, 98))\n",
    "    \n",
    "    bp2, bp98 = np.percentile(b, (2, 98))\n",
    "\n",
    "    return(rp2, rp98, gp2, gp98, bp2, bp98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rgb(rgb, rp2, rp98, gp2, gp98, bp2, bp98, red, green, blue, index):\n",
    "\n",
    "    r = np.nan_to_num(np.array(rgb.data_vars[red][index]))\n",
    "    g = np.nan_to_num(np.array(rgb.data_vars[green][index]))\n",
    "    b = np.nan_to_num(np.array(rgb.data_vars[blue][index]))\n",
    "\n",
    "    r_rescale = exposure.rescale_intensity(r, in_range=(rp2, rp98))\n",
    "    g_rescale = exposure.rescale_intensity(g, in_range=(gp2, gp98))\n",
    "    b_rescale = exposure.rescale_intensity(b, in_range=(bp2, bp98))\n",
    "\n",
    "    rgb_stack = np.dstack((r_rescale,g_rescale,b_rescale))\n",
    "    img = img_as_float(rgb_stack)\n",
    "\n",
    "    return(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### include an index here for the timeslice with representative data for best stretch of time series\n",
    "# don't run this to keep the same limits as the previous sensor\n",
    "rp2, rp98, gp2, gp98, bp2, bp98 = get_p2_p98(sr_cloudfree,landsat_rgb_params[sensor]['red'],landsat_rgb_params[sensor]['green'],landsat_rgb_params[sensor]['blue'], 1)\n",
    "rp2, rp98, gp2, gp98, bp2, bp98 = (300.0, 2000.0, 300.0, 2000.0, 300.0, 2000.0)\n",
    "print(rp2, rp98, gp2, gp98, bp2, bp98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from IPython.display import clear_output\n",
    "non_decimal = re.compile(r'[^\\d.]+')\n",
    "index = 0\n",
    "\n",
    "for timeslice in sr_cloudfree.time.to_dict()['data']:\n",
    "    print timeslice\n",
    "    time_filename = non_decimal.sub('', str(timeslice))\n",
    "    plt.figure(num=None, dpi=600, facecolor='w')\n",
    "    #plt.figsize=(10.0, 10.0)\n",
    "    #plt.annotate(str(timeslice)+' '+sensor, xy=(100, 100), xytext=(10, 20), color='red')\n",
    "    #plt.tight_layout()\n",
    "    plt.imshow(plot_rgb(sr_cloudfree,rp2, rp98, gp2, gp98, bp2, bp98,landsat_rgb_params[sensor]['red'],\\\n",
    "                        landsat_rgb_params[sensor]['green'], landsat_rgb_params[sensor]['blue'], index),interpolation='nearest')\n",
    "\n",
    "    plt.savefig(time_filename+'.png', transparent=True, bbox_inches='tight', \\\n",
    "                        pad_inches=0, dpi=600)\n",
    "    index = index+1\n",
    "    plt.close()\n",
    "    clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "import numpy as np\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "nav_menu": {},
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "335px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
